# EMPIRIC_TC

Description here

Some figs


## Set Up

1. Install Python if you don’t already have it. I am using python version 3.11.9. I would recommend using the this version because there are some issues with using the most recent Python release. You can find the installers here: https://www.python.org/downloads/release/python-3119/
2. Install git: https://git-scm.com/download/win for windows or https://git-scm.com/download/mac for mac. On windows, if it has an option in the install like “Use git in Windows command prompt”, select yes. (I haven’t used git on windows)
3. Get the latest release of the code locally by opening the command line and running: `git clone https://github.com/EMPIRIC2/EMPIRIC-AI-emulation.git --branch v0.1.0-beta` 
    1. You’ll need to do some authentication for your github account, you can do this by making a personal access token through your github account here: https://github.com/settings/tokens. There is also a guide on this here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
    2. this will put the project folder in your current working directory, so make sure to move to the directory you want to use in the command line
4.  Move into the code folder: `cd EMPIRIC-AI-emulation` 
5. Set up the conda environment: `conda env create -f env.yml`
6. Activate the conda environment: `conda activate ml_env`
7. Download the model and test data files from the manuscript release version: [ADD LINK HERE]
8. Move the saved model ([UPDATE]) you have downloaded into the folder EMPIRIC-AI-emulation/saved_models
9. To run the STORM model directly, you must download the storm_data.zip file from the most recent release (see ^), unzip it, and add all the files to the EMPIRIC-AI-emulation/STORM folder.

## Running the Model

Saved models are stored in `./saved_models/` and can be loaded from `saved_models.saved_models`. 

The three current models are DDPMUNet_model, STORM_model and NearestNeighbors_model.
Raw input data can be loaded from HDF5 files using the `raw_input_generator("path/to/data/file.hdf5")`.

Otherwise, models all expect a (6, 55, 105) float-valued numpy ndarray representing the relative 
spatial frequency of genesis events (the scale does not matter 
since they are normalized before inputting to the deep learning model).

For example, this code shows evaluating an input for both the deep learning model and STORM.

```python
from saved_models.saved_models import DDPMUNet_model
model = DDPMUNet_model()

# for a model input x, x is a (6, 55, 105) numpy array
y = model(x)
# get a model prediction y, y is a (110, 210, 1) numpy array

# Or

from saved_models.saved_models import STORM_model
model = STORM_model(total_years=1, n_years_to_sum=1, n_samples=1)

y = model(x)
```

There is an example of this in the notebook `example_code/prediction_example.ipynb`. 
To run the example code, update the data_file path to point to the file "example_data.hdf5" that you downloaded from the release.

## Generating synthetic tropical cyclone data


## Evaluating the models


## Figures

Figures are generated by the model evaluation script and in individual jupyter notebooks.

[For each figure in the manuscript, reference the notebook used to generate it]

## Citing


